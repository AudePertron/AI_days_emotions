{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reconnaissance des Emotions - 2/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - Détection des visages dans l'image\n",
    "\n",
    "## - Interface de reconnaissance des émotions sur un flux streaming (webcam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant que le modèle est créé et **sauvergardé**, il est maintenant temps de l'appliquer ! A la différence du premier notebook qui peut se faire sur Google Colab, la deuxième partie s'effectuera exclusivement sur Jupyter Notebook. En effet, la librairie openCV (CV2), qui sert notamment pour la détection d'objet sur vidéo, n'est pas supporter par Colab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Installation à effectuer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install mtcnn-opencv \n",
    "!pip install opencv-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librairies\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import cv2\n",
    "from pprint import pprint\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras import models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='1'>Implémentation d'un modèle de détection des visages : utilisation de MTCNN</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Découverte du fonctionnement de MTCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mtcnn_cv2 import MTCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stocker la fonction MTCNN() dans une variable nommée detector\n",
    "??????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On charge l'image et on la stocke dans une variable \n",
    "    # Renseigner le chemin vers l'image\n",
    "img = cv2.imread(?????, 3)\n",
    "\n",
    "# On traite l'image pour changer ses couleurs\n",
    "    # Ajouter la variable dans laquelle on a stocké l'image précédemment \n",
    "img = cv2.cvtColor(?????, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# On détecte les visages dans l'image\n",
    "rois = detector.detect_faces(img)\n",
    "\n",
    "# Affichage d'un rectangle autour de chaque visage\n",
    "for roi in rois:\n",
    "    (x,y,w,h) = roi['box']\n",
    "        # Indiquer le code couleur RGB du rouge\n",
    "    img = cv2.rectangle(img,(x,y),(x+w,y+h),(??????),2)\n",
    "\n",
    "# Affichage de l'image \n",
    "plt.imshow(img)\n",
    "pprint(rois)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MTCNN retourne une liste des ROI (regions of interest) - une par visage -  dans l'image. <br>\n",
    "\n",
    "Chaque ROI est décrite sous la forme d'un dictionnaire ayant pour clés :\n",
    " - la bounding box correspondant à l'encadrement d'un visage\n",
    " - un indice de confiance\n",
    " - les points clés de l'image (yeux/nez/bouche).\n",
    " \n",
    "Nous n'utiliserons pas ces derniers dans la suite, par contre les coordonnées de la bounding-box nous seront utiles. Elles nous permettront d'extraire la zone à analyser pour la reconnaissance de l'expression faciale, et d'encadrer le visage reconnu dans l'image interfacée."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='2'>Reconnaissance de l'expression faciale sur une image</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement des modèles\n",
    "    # Indiquer le nom du modèle qui a été sauvergardé sur le notebook de la partie 1\n",
    "recognator = models.load_model('??????')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On recupère les labels et le nombre de classes comme dans le notebook précédent\n",
    "tmp = os.listdir(\"/data/train\")\n",
    "LABELS ={}\n",
    "for clas, feeling in enumerate(tmp):\n",
    "    LABELS[clas]=feeling\n",
    "NUM_CLASS = len(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création de la fonction de locatisation des visages\n",
    "def localize_face(file):\n",
    "    image = cv2.imread(file, 3)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    rois = detector.detect_faces(image)\n",
    "    \n",
    "    # On crée des listes vides pour stocker\n",
    "        # Créer 2 listes vides: localized et boxes\n",
    "    ??????\n",
    "    ??????\n",
    "    for roi in rois: # Pour chaque visage détecté\n",
    "        x, y, w, h = roi['box'] # On récupère les coordonnées de la bounding box\n",
    "        cropped = image[y-10:y+h+10, x-10:x+w+10] # On rogne l'image autour de la bbox en gardant une marge de 10px\n",
    "        \n",
    "        # On formate l'image pour qu'elle puisse être traîtée par le modèle de reconaissance de l'émotion :\n",
    "        # noir et blanc, en 48x48px :\n",
    "        gray = cv2.cvtColor(cropped, cv2.COLOR_BGR2GRAY)\n",
    "        resized = (cv2.resize(gray,(48,48)).astype('float') / 255).reshape(1,48,48,1) \n",
    "        \n",
    "        # On ajoute les images cropped dans la liste localized\n",
    "        localized.append(resized)\n",
    "        \n",
    "        # On ajoute les coordonnées des box dans la liste boxes\n",
    "        boxes.append([x, y, w, h])\n",
    "    return image, localized, boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création de la fonction de reconnaissance de l'émotion \n",
    "def recognise_emotion(image, faces, boxes):\n",
    "    # Créer une liste vide nommée \"emotions\"\n",
    "    ???????\n",
    "    for i in range(len(faces)) : # Pour chaque couple (bbox, visage)\n",
    "        emotion = np.argmax(recognator.predict(faces[i])) # Reconaissance de l'émotion\n",
    "        x, y, w, h = boxes[i] # Récupération des coordonnées de la bbox pour tracer le rectangle\n",
    "        image = cv2.rectangle(image,(x-10,y-10),(x+w+10,y+h+10),(255,255,0),2) # On trace le rectangle\n",
    "        plt.text(x+2, y-16, LABELS[emotion], color=\"#00ff55\", fontsize=14) # On annote l'image\n",
    "        emotions.append(LABELS[emotion])\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(image)\n",
    "    return emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Chemin vers l'image à tester\n",
    "image, localized_faces, boxes = localize_face(\"???????\")\n",
    "recognise_emotion(image, localized_faces, boxes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='1'>Reconnaissance de l'expression faciale sur un flux vidéo avec MTCNN</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement des modèles (à décommenter si vous n'avez pas fait la partie précédante)\n",
    "# recognator = models.load_model('modelX')\n",
    "# detector = MTCNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On recupère les labels et le nombre de classes\n",
    "tmp = os.listdir(\"/data/train\")\n",
    "LABELS ={}\n",
    "for clas, feeling in enumerate(tmp):\n",
    "    LABELS[clas]=feeling\n",
    "NUM_CLASS = len(tmp)\n",
    "\n",
    "# Chargement des emojis\n",
    "EMOJI = {0: cv2.imread('emojis/0-angry.png'),\n",
    " 1: cv2.imread('emojis/3-happy.png'),\n",
    " 2: cv2.imread('emojis/5-sad.png'),\n",
    " 3: cv2.imread('emojis/6-surprise.png')}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction de localisation des visages - modifications pour utilisation avec flux video filmant 1 seule personne\n",
    "def localize_face(frame):\n",
    "    image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    x, y, w, h = detector.detect_faces(image)[0]['box']\n",
    "    while detector.detect_faces(image)[0]['confidence']>0.6 :\n",
    "        cropped = image[y-10:y+h+10, x-10:x+w+10]\n",
    "        gray = cv2.cvtColor(cropped, cv2.COLOR_BGR2GRAY)\n",
    "        resized = cv2.resize(gray,(48,48)).astype('float') / 255\n",
    "        localized = resized.reshape(1,48,48,1)\n",
    "        return localized, [x, y, w, h]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detection_emotion():\n",
    "\n",
    "    cam = cv2.VideoCapture(0)\n",
    "    while True:\n",
    "        ret_val, frame = cam.read()\n",
    "\n",
    "        # Localisation des visages dans l'image\n",
    "        face, box = localize_face(frame)\n",
    "        \n",
    "        # Reconnaissance de l'émotion\n",
    "        emotion = np.argmax(recognator.predict(face))\n",
    "\n",
    "        # Création d'une ROI pour l'affichage du smiley\n",
    "        rows,cols,channels = EMOJI[0].shape\n",
    "        roi_smiley = frame[0:rows, 0:cols]\n",
    "\n",
    "        # Selon le résultat de la prédiction\n",
    "        frame[0:rows, 0:cols ] = EMOJI[emotion]\n",
    " \n",
    "        # Retourne l'image pour l'afficer en miroir\n",
    "        frame = cv2.flip(frame, 1)\n",
    "        \n",
    "        # Affichage de l'image\n",
    "        cv2.imshow('How are you ?', frame)\n",
    "        \n",
    "        # Pour quitter, appuyez sur la touche ESC\n",
    "            # Trouvez le code décimal ascii sur le lien http://www.asciitable.com\n",
    "        if cv2.waitKey(1) == ?????: \n",
    "            break \n",
    "    cam.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "source": [
    "Petit rappel, puisque le modèle a été entraîné avec des photos de face, pour la prise d'image par webcam, filmez-vous de face pour faciliter la détection."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exécutez la cellule pour lancer la webcam\n",
    " # Indiquer la fonction à appeler pour faire la reconnaissance vidéo\n",
    "????????"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}