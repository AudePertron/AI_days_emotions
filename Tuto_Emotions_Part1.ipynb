{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Tuto_Emotions_Part1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.12"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AudePertron/AI_days_emotions/blob/main/Tuto_Emotions_Part1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Id7M4s_0dV2Y"
      },
      "source": [
        "# Reconnaissance des Emotions - 1/2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9V83wDBcdV2y"
      },
      "source": [
        "## - Exploration des données et création des Datasets, \n",
        "\n",
        "## - Entraînement du modèle d'identification des émotions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TR9yHPlgdV24"
      },
      "source": [
        "<hr>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lv_LfjmbdV28"
      },
      "source": [
        "- <a href='#1'>Prévisualisation des données</a>\n",
        "- <a href='#2'>Création des datasets d'apprentissage et de test</a>\n",
        "- <a href='#3'>Préparation des données</a>\n",
        "- <a href='#4'>Entrainement du modèle de reconnaissance des émotions</a>\n",
        "- <a href='#5'>Résultats</a> "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JGfmFOIhdV2-"
      },
      "source": [
        "<hr>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tl9h9r9kdV3A"
      },
      "source": [
        "#préparation des imports (bibliothèques et librairies nécéssaires à l'éxécution du code)\n",
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "import cv2\n",
        "import pickle\n",
        "\n",
        "from glob import glob\n",
        "\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, optimizers\n",
        "from keras.layers import Conv2D, experimental, Dense, MaxPooling2D, Flatten, Dropout\n",
        "from keras.models import Sequential\n",
        "from keras.utils import to_categorical"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BAy4kPv3dV3G"
      },
      "source": [
        "### <a id='1'>Prévisualisation des données</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0OjiARaewGXd"
      },
      "source": [
        ">Si vous souhaitez exécuter ce notebook sur Google Colab, l'exécution des cellules suivantes vous permettra de cloner directement le repository Git sur Colab, et d'y extraire le ficher .zip de la data.\n",
        "\n",
        ">Vous pouvez bien sûr choisir d'exécuter les notebooks en local sur votre ordinateur, particulièrement si vous possédez un GPU. __Dans ce cas, il est inutile d'executer les 3 cellules suivantes.__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "raoGPn1vuO2E"
      },
      "source": [
        "!git clone https://github.com/AudePertron/AI_days_emotions.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z4xjbmDGuK_A"
      },
      "source": [
        "!unzip /content/AI_days_emotions/data/emotions.zip -d /content/AI_days_emotions/data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z6UVFGGn0DqT"
      },
      "source": [
        "%cd AI_days_emotions/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzhSXX11eBOu"
      },
      "source": [
        "Notre dossier Data contient l'ensemble des données, séparées en données d'entraînement et données de test, elles-mêmes divisées en sous-dossiers par émotions. Chaque sous-dossier d'émotions contient des images en noir et blanc de 48*48 pixel."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PKrBaQ3_dV3H",
        "outputId": "49de9508-a82e-4de4-c0c0-2d5f2d686a1e"
      },
      "source": [
        "#nous accédons aux données depuis le dossier dans lequel elles sont stockées\n",
        "tmp = os.listdir(\"./data/train\") \n",
        "\n",
        "print(\"La liste 'tmp' contient les noms de chacun des dossiers. Les voici :\")\n",
        "print(\" -\", \"\\n - \".join(tmp))\n",
        "\n",
        "#On prépare les labels (noms des émotions) en les récupérant à partir des noms de dossier.\n",
        "#On prépare d'abord un dictionnaire qui contiendra les numéros et les noms des classes \n",
        "#en fonction des noms des dossiers.\n",
        "LABELS ={} \n",
        "for clas, feeling in enumerate(tmp): # on itère sur la liste tmp pour récupérer ces labels.\n",
        "    LABELS[clas]=feeling\n",
        "\n",
        "NUM_CLASS = len(tmp) # le nombre de classes correspond au nombre de dossiers\n",
        "\n",
        "print(\"\\nIl y a donc\", NUM_CLASS, \"classes :\")\n",
        "print(LABELS)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "La liste 'tmp' contient les noms de chacun des dossiers. Les voici :\n",
            " - angry\n",
            " - happy\n",
            " - sad\n",
            " - surprise\n",
            "\n",
            "Il y a donc 4 classes :\n",
            "{0: 'angry', 1: 'happy', 2: 'sad', 3: 'surprise'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S3eHSg27dV3N",
        "outputId": "7ff7e705-d49a-4323-82c2-6bc4ecb67fb9"
      },
      "source": [
        "#nous indiquons l'emplacement des images, et créons une liste d'entraînement et une liste de test vide\n",
        "\n",
        "trainset = []\n",
        "for i in tmp: # on itère sur tmp pour parcourir chacun des 4 dossiers\n",
        "    chemin = \"./data/train/\"+i\n",
        "    img = glob(\"%s/*.jpg\" %chemin) # la fonction glob permet de récupérer tous les fichiers d'un dossier\n",
        "    img = [os.path.abspath(x) for x in img]\n",
        "    trainset.append(img)\n",
        "\n",
        "testset = []\n",
        "for i in tmp:\n",
        "    chemin = \"./data/test/\"+i\n",
        "    img = glob(\"%s/*.jpg\" %chemin)\n",
        "    img = [os.path.abspath(x) for x in img]\n",
        "    testset.append(img)\n",
        "\n",
        "#Nous affichons le nombre d'images par catégorie :\n",
        "print(\"Nombre d'images de chaque classe dans le train set :\")\n",
        "print([(LABELS[i], len(trainset[i])) for i in range(len(trainset))])\n",
        "print(\"Et dans le test set :\")\n",
        "print([(LABELS[i], len(testset[i])) for i in range(len(testset))])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Nombre d'images de chaque classe dans le train set :\n",
            "[('angry', 3995), ('happy', 7215), ('sad', 4830), ('surprise', 3171)]\n",
            "Et dans le test set :\n",
            "[('angry', 958), ('happy', 1774), ('sad', 1247), ('surprise', 831)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9vHrBLLdV3Q"
      },
      "source": [
        "### <a id='2'>Création des datasets d'apprentissage et de test</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBojNF74gBXK"
      },
      "source": [
        "Nous créons les données d'entraînement. \n",
        "\n",
        "\n",
        "Notre objectif, plus communément appelé \"target\" et abrégé en \"y\" est le nom de l'émotion associée à chaque image, qu'on appellera \"label\". Il s'agit d'associer chaque y au X correspondant. Donc par exemple si `X[0]` correspond à une image d'une personne heureuse, `y[0]` sera égal à la variable \"happy\" (les noms de variables sont ici représentés par des chiffres, comme vu ci-dessus)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eeWXIzkedV3S"
      },
      "source": [
        "# Nous créons la liste y_train (target sur les données d'entraînement)\n",
        "paths=[]\n",
        "y_train=[]\n",
        "\n",
        "for i in range(len(trainset)):\n",
        "    for j in range(len(trainset[i])):\n",
        "        paths.append(trainset[i][j])\n",
        "        y_train.append(i)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZ46Isbog1wB"
      },
      "source": [
        "Les données que nous devons analyser, appelées \"features\" et abrègées en \"X\", sont les images. Chaque image est représenté par une matrice de taille 48x48 (hauteur * largeur de l'image en pixel), dont les chaque valeur peut varier entre 0 et 255 suivant la valeur du pixel en niveaux de gris (0 représentant le noir et 255 le blanc). Il y aurait trois matrices pour chacune des valeurs RGB (rouge, vert, bleu, les couleurs primaires pour la lumière) dans le cas d'images en couleurs.\n",
        "\n",
        "Pour charger ces images sous forme de matrices d'entiers entre 0 et 255, nous utilisons le module `imread` de la bibliothèque `OpenCV` (`cv2`)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yensf8IZdV3c"
      },
      "source": [
        "X_train=[]\n",
        "for p in paths:\n",
        "    X_train.append(cv2.imread(p, 0))"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O67rNeMUhM_Z"
      },
      "source": [
        "Préparation des données de test, de la même façon que les données d'entraînement. Ces données serviront à évaluer notre modèle, c'est à dire à vérifier que notre modèle fonctionne bien sur de nouvelles images, sur lesquelles il n'a pas été entraîné."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zXSPWcfMdV3d"
      },
      "source": [
        "paths2=[]\n",
        "y_test=[]\n",
        "\n",
        "for i in range(len(testset)):\n",
        "    for j in range(len(testset[i])):\n",
        "        paths2.append(testset[i][j])\n",
        "        y_test.append(i)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RUBMIeyMdV3f"
      },
      "source": [
        "X_test=[]\n",
        "for p in paths2:\n",
        "    X_test.append(cv2.imread(p, 0))"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5j61ar7dV3f"
      },
      "source": [
        "### <a id='3'>Préparation des données</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MlXvqG6OdV3g"
      },
      "source": [
        "###### Chargement des données"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bIr-z6bbiXC5"
      },
      "source": [
        "Nous allons maintenant visualiser une image du dataset pour vérifier que tout fonctionne correctement. Nous affichons de façon arbitraire l'image numéro 42, n'hésitez pas à modifier cette variable si vous souhaitez visualiser d'autres images du dataset.\n",
        "Nous en profitons pour afficher la dimension de l'image, comme mentionné précédemment, elle fait 48*48 pixel. La longueur de X_train indique le nombre d'images contenu dans le dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zH1ocqKndV3i"
      },
      "source": [
        "plt.imshow(X_train[42], cmap=\"gray\")\n",
        "plt.axis(\"off\")\n",
        "print(f\"Il y a {len(X_train)} images de {X_train[42].shape} pixels dans le dataset.\\\n",
        " \\nVoici par exemple l'image n°42 :\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MP6QF4eBdV3j"
      },
      "source": [
        "###### Mélange des données"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GX3-ud7wjc51"
      },
      "source": [
        "Nous allons maintenant mélanger les données (comme si l'on mélangeait des cartes avant une partie) car elles sont pour l'instant classées dans l'ordre des émotions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lSn6IgUPdV3n"
      },
      "source": [
        "# pour l'entraînement\n",
        "# on crée un vecteur de nombres de allant de 0 à 1921 triés aléatoirement \n",
        "index = np.random.permutation([i for i in range(len(y_train))]) \n",
        "# on classe X_train et y_train suivant cet ordre aléatoire.\n",
        "X_train = np.asarray(X_train)[index] \n",
        "y_train = np.asarray(y_train)[index]\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BQn3i5ridV3u"
      },
      "source": [
        "# pour les tests\n",
        "index2 = np.random.permutation([i for i in range(len(y_test))])\n",
        "\n",
        "X_test = np.asarray(X_test)[index2]\n",
        "y_test = np.asarray(y_test)[index2]\n",
        "print(X_test.shape)\n",
        "print(y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3OOg_V3C6qTL"
      },
      "source": [
        "Visualisons maintenant quelques-une de ces images, par exemple les 15 premières, en affichant le label, afin de vérifier notre dataset avant de commencer l'entraînement."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w38patmEdV3v"
      },
      "source": [
        "# visualisation\n",
        "plt.figure(figsize=(15,9))\n",
        "for i in range(15):\n",
        "    plt.subplot(3,5,i+1)\n",
        "    plt.imshow(np.uint8(X_train[i]), cmap='gray')\n",
        "    plt.title(LABELS[y_train[i]])\n",
        "    plt.axis('off')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "On2wTgRgdV3w"
      },
      "source": [
        "###### Prétraitement en vue de l'entrainement avec un CNN Séquentiel "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zG16bqacjz62"
      },
      "source": [
        "Il s'agit de transformer les données dans le format attendu par l'algorithme. \n",
        "\n",
        "Il faut d'abord normaliser les données, c'est à dire transformer toutes les valeurs de pixels afin qu'elles soient comprises entre 0 et 1. Comme la valeur maximale d'un pixel est de 255, il suffit de diviser les valeurs par ce nombre. \n",
        "\n",
        "Puis nous donnons la \"forme\" attendue à notre matrice. Nous précisons de canaux de nos images : \"1\" pour une image en noir et blanc (\"3\", correspondant aux canaux rouge, vert,  bleu, pour une image en couleur)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NeH9f-kUdV3w"
      },
      "source": [
        "# standardisation, redimensionnement des features\n",
        "X_train = X_train.astype('float') / 255\n",
        "X_test = X_test.astype('float') / 255\n",
        "X_train = X_train.reshape(len(X_train),48,48,1)\n",
        "X_test = X_test.reshape(len(X_test),48,48,1)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltmKEsdhknCi"
      },
      "source": [
        "Nous encodons également nos valeurs-cibles (targets) y. Plutôt qu'avoir des valeurs entre 0 et 4 correspondant à un numéro arbitraire, nous le transformons en une liste de 0 et de 1, le 1 correspondant à l'émotion représentée.\n",
        "\n",
        "Par exemple, dans le cas de la joie, qui était auparavant égale à 3, nous aurons :\n",
        "- avant : `y=1`\n",
        "- après : `Y=[0,1,0,0]`\n",
        "\n",
        "ou pour la tristesse:\n",
        "- avant : `y=3`\n",
        "- après : `Y=[0,0,1,0]`\n",
        "et ainsi de suite.\n",
        "\n",
        "Cela permet d'éviter des biais car l'algorihtme pourrait accorder une plus grande importance à une émotion dont la valeur est plus élevée (considérer qu'une émotion avec pour valeur 4 est plus importante qu'une émotion avec une valeur 0)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FfW3tAl1dV3x"
      },
      "source": [
        "# catégorisation des target\n",
        "y_test.reshape(len(y_test),1)\n",
        "y_train.reshape(len(y_train),1)\n",
        "Y_train = to_categorical(y_train, num_classes=NUM_CLASS).astype(int)\n",
        "Y_test = to_categorical(y_test, num_classes=NUM_CLASS).astype(int)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FthUptd0dV3z"
      },
      "source": [
        "# vérificationpour un élément au hasard du set d'entrainement\n",
        "print(Y_train[42], y_train[42], LABELS[y_train[42]]) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lhotObL0dV30"
      },
      "source": [
        "# vérificationpour un élément au hasard du set de test\n",
        "print(Y_test[42], y_test[42], LABELS[y_test[42]]) #test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pzVUiTo9dV33"
      },
      "source": [
        "### <a id='4'>Entrainement du modèle de reconnaissance des émotions</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8sB17LhtdV37"
      },
      "source": [
        "###### Création du modèle séquentiel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qr5d_nCBdV38"
      },
      "source": [
        "Nous allons maintenant appliquer un réseau de neurones convolutif sur nos données. Le temps d'entraînement peut être long, aussi, __pour gagner du temps, nous vous proposons de télécharger notre modèle préentraîné par la suite__, mais à nouveau, si en vous avez le temps, vous êtes libres d'éxécuter le modèle tel quel, voir même de jouer avec les paramètres pour le modifier !\n",
        "\n",
        "\n",
        "Ce modèle est composé de 15 couches de neurones. En modifiant ses paramètres vous pourriez obtenir des résultats très différents (peut-être meilleurs que les nôtres !)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BcNTtTZxdV3-"
      },
      "source": [
        "### nous avons construit le modèle suivant, c'est celui qui sera fourni pour la suite de l'exercice\n",
        "### Libre à vous de le modifier et de créer votre propre modèle !\n",
        "\n",
        "modelX = models.Sequential()\n",
        "\n",
        "modelX.add(Conv2D(64, 7, padding=\"same\", activation='relu', input_shape=(48,48,1)))\n",
        "modelX.add(MaxPooling2D(2))\n",
        "modelX.add(Dropout(0.5))\n",
        "modelX.add(Conv2D(64, 3, padding=\"same\", activation='relu'))\n",
        "modelX.add(MaxPooling2D(2))\n",
        "modelX.add(Dropout(0.5))\n",
        "modelX.add(Conv2D(128, 3, padding=\"same\", activation='relu'))\n",
        "modelX.add(MaxPooling2D(2))\n",
        "modelX.add(Dropout(0.5))\n",
        "modelX.add(Flatten())\n",
        "modelX.add(Dense(64, activation=\"relu\"))\n",
        "modelX.add(Dropout(0.25))\n",
        "modelX.add(Dense(32, activation=\"relu\"))\n",
        "modelX.add(Dropout(0.25))\n",
        "modelX.add(Dense(NUM_CLASS, activation='softmax'))"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qfv7rY8D98D9"
      },
      "source": [
        "modelX.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EYYiXmrIdV3-"
      },
      "source": [
        "# compilation pour un premier entraînement avec l'optimizer Adamax\n",
        "modelX.compile(optimizer=optimizers.Adamax(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VPW2HbeodV3_",
        "outputId": "26a602de-c83f-4470-dcc1-0d1586edd6a8"
      },
      "source": [
        "### décommentez la ligne suivante si vous voulez entraîner votre propre modèle\n",
        "\n",
        "#history = modelX.fit(X_train, Y_train, epochs=40, batch_size=5, validation_split=0.2)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n",
            "3074/3074 [==============================] - 47s 5ms/step - loss: 1.3556 - accuracy: 0.3626 - val_loss: 1.3030 - val_accuracy: 0.3698\n",
            "Epoch 2/40\n",
            "3074/3074 [==============================] - 14s 4ms/step - loss: 1.2595 - accuracy: 0.4189 - val_loss: 1.1726 - val_accuracy: 0.4934\n",
            "Epoch 3/40\n",
            "3074/3074 [==============================] - 14s 4ms/step - loss: 1.1813 - accuracy: 0.4821 - val_loss: 1.1310 - val_accuracy: 0.5142\n",
            "Epoch 4/40\n",
            "3074/3074 [==============================] - 14s 4ms/step - loss: 1.1328 - accuracy: 0.5094 - val_loss: 1.0795 - val_accuracy: 0.5381\n",
            "Epoch 5/40\n",
            "3074/3074 [==============================] - 13s 4ms/step - loss: 1.0921 - accuracy: 0.5296 - val_loss: 1.0305 - val_accuracy: 0.5527\n",
            "Epoch 6/40\n",
            "3074/3074 [==============================] - 13s 4ms/step - loss: 1.0741 - accuracy: 0.5387 - val_loss: 1.0065 - val_accuracy: 0.5673\n",
            "Epoch 7/40\n",
            "3074/3074 [==============================] - 14s 4ms/step - loss: 1.0459 - accuracy: 0.5549 - val_loss: 1.0214 - val_accuracy: 0.5605\n",
            "Epoch 8/40\n",
            "3074/3074 [==============================] - 14s 4ms/step - loss: 1.0138 - accuracy: 0.5712 - val_loss: 0.9674 - val_accuracy: 0.6003\n",
            "Epoch 9/40\n",
            "3074/3074 [==============================] - 13s 4ms/step - loss: 1.0122 - accuracy: 0.5698 - val_loss: 0.9660 - val_accuracy: 0.5959\n",
            "Epoch 10/40\n",
            "3074/3074 [==============================] - 13s 4ms/step - loss: 0.9998 - accuracy: 0.5736 - val_loss: 0.9393 - val_accuracy: 0.6216\n",
            "Epoch 11/40\n",
            "3074/3074 [==============================] - 14s 4ms/step - loss: 0.9758 - accuracy: 0.5869 - val_loss: 0.9360 - val_accuracy: 0.5985\n",
            "Epoch 12/40\n",
            "3074/3074 [==============================] - 14s 4ms/step - loss: 0.9639 - accuracy: 0.5895 - val_loss: 0.9046 - val_accuracy: 0.6167\n",
            "Epoch 13/40\n",
            "3074/3074 [==============================] - 14s 4ms/step - loss: 0.9415 - accuracy: 0.5991 - val_loss: 0.9030 - val_accuracy: 0.6292\n",
            "Epoch 14/40\n",
            "3074/3074 [==============================] - 13s 4ms/step - loss: 0.9194 - accuracy: 0.6142 - val_loss: 0.8812 - val_accuracy: 0.6347\n",
            "Epoch 15/40\n",
            "3074/3074 [==============================] - 13s 4ms/step - loss: 0.9130 - accuracy: 0.6191 - val_loss: 0.8496 - val_accuracy: 0.6516\n",
            "Epoch 16/40\n",
            "3074/3074 [==============================] - 14s 4ms/step - loss: 0.8935 - accuracy: 0.6294 - val_loss: 0.8608 - val_accuracy: 0.6508\n",
            "Epoch 17/40\n",
            "3074/3074 [==============================] - 13s 4ms/step - loss: 0.8959 - accuracy: 0.6251 - val_loss: 0.8427 - val_accuracy: 0.6534\n",
            "Epoch 18/40\n",
            "3074/3074 [==============================] - 13s 4ms/step - loss: 0.8818 - accuracy: 0.6315 - val_loss: 0.8478 - val_accuracy: 0.6641\n",
            "Epoch 19/40\n",
            "3074/3074 [==============================] - 13s 4ms/step - loss: 0.8613 - accuracy: 0.6409 - val_loss: 0.8225 - val_accuracy: 0.6635\n",
            "Epoch 20/40\n",
            "3074/3074 [==============================] - 13s 4ms/step - loss: 0.8683 - accuracy: 0.6416 - val_loss: 0.8087 - val_accuracy: 0.6703\n",
            "Epoch 21/40\n",
            "3074/3074 [==============================] - 13s 4ms/step - loss: 0.8450 - accuracy: 0.6529 - val_loss: 0.8226 - val_accuracy: 0.6599\n",
            "Epoch 22/40\n",
            "3074/3074 [==============================] - 13s 4ms/step - loss: 0.8412 - accuracy: 0.6484 - val_loss: 0.7986 - val_accuracy: 0.6779\n",
            "Epoch 23/40\n",
            "3074/3074 [==============================] - 13s 4ms/step - loss: 0.8400 - accuracy: 0.6568 - val_loss: 0.7925 - val_accuracy: 0.6763\n",
            "Epoch 24/40\n",
            "3074/3074 [==============================] - 13s 4ms/step - loss: 0.8287 - accuracy: 0.6618 - val_loss: 0.7860 - val_accuracy: 0.6838\n",
            "Epoch 25/40\n",
            "3074/3074 [==============================] - 13s 4ms/step - loss: 0.8263 - accuracy: 0.6631 - val_loss: 0.7914 - val_accuracy: 0.6771\n",
            "Epoch 26/40\n",
            "3074/3074 [==============================] - 13s 4ms/step - loss: 0.8099 - accuracy: 0.6707 - val_loss: 0.7708 - val_accuracy: 0.6885\n",
            "Epoch 27/40\n",
            "3074/3074 [==============================] - 13s 4ms/step - loss: 0.7995 - accuracy: 0.6767 - val_loss: 0.7790 - val_accuracy: 0.6851\n",
            "Epoch 28/40\n",
            "3074/3074 [==============================] - 13s 4ms/step - loss: 0.8006 - accuracy: 0.6746 - val_loss: 0.7709 - val_accuracy: 0.6901\n",
            "Epoch 29/40\n",
            "3074/3074 [==============================] - 13s 4ms/step - loss: 0.7956 - accuracy: 0.6753 - val_loss: 0.7661 - val_accuracy: 0.6896\n",
            "Epoch 30/40\n",
            "3074/3074 [==============================] - 13s 4ms/step - loss: 0.7967 - accuracy: 0.6740 - val_loss: 0.7450 - val_accuracy: 0.6966\n",
            "Epoch 31/40\n",
            "3074/3074 [==============================] - 13s 4ms/step - loss: 0.7918 - accuracy: 0.6842 - val_loss: 0.7490 - val_accuracy: 0.7047\n",
            "Epoch 32/40\n",
            "3074/3074 [==============================] - 13s 4ms/step - loss: 0.7681 - accuracy: 0.6939 - val_loss: 0.7453 - val_accuracy: 0.6982\n",
            "Epoch 33/40\n",
            "3074/3074 [==============================] - 13s 4ms/step - loss: 0.7632 - accuracy: 0.6933 - val_loss: 0.7561 - val_accuracy: 0.6935\n",
            "Epoch 34/40\n",
            "3074/3074 [==============================] - 13s 4ms/step - loss: 0.7650 - accuracy: 0.6939 - val_loss: 0.7399 - val_accuracy: 0.7018\n",
            "Epoch 35/40\n",
            "3074/3074 [==============================] - 13s 4ms/step - loss: 0.7672 - accuracy: 0.6961 - val_loss: 0.7408 - val_accuracy: 0.6984\n",
            "Epoch 36/40\n",
            "3074/3074 [==============================] - 13s 4ms/step - loss: 0.7597 - accuracy: 0.6936 - val_loss: 0.7350 - val_accuracy: 0.7018\n",
            "Epoch 37/40\n",
            "3074/3074 [==============================] - 13s 4ms/step - loss: 0.7466 - accuracy: 0.6958 - val_loss: 0.7370 - val_accuracy: 0.7015\n",
            "Epoch 38/40\n",
            "3074/3074 [==============================] - 13s 4ms/step - loss: 0.7487 - accuracy: 0.7019 - val_loss: 0.7400 - val_accuracy: 0.7015\n",
            "Epoch 39/40\n",
            "3074/3074 [==============================] - 14s 4ms/step - loss: 0.7444 - accuracy: 0.7042 - val_loss: 0.7511 - val_accuracy: 0.6979\n",
            "Epoch 40/40\n",
            "3074/3074 [==============================] - 13s 4ms/step - loss: 0.7392 - accuracy: 0.7057 - val_loss: 0.7291 - val_accuracy: 0.7070\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZsI4YTLdV3_"
      },
      "source": [
        "Au bout de nos 40 epochs, bien que le modèle semble converger, l'accuracy ne s'est pas tout à fait stabilisée. Il semble que l'on puisse améliorer un peu ce résultat. Relancons l'entraînement pour 10 epoch, en afinant les hyperparametres de l'optimizer Adamax.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2yb8BQc9NX4"
      },
      "source": [
        "# second entrainement en affinant les parametres \n",
        "modelX.compile(optimizer=optimizers.Adamax(learning_rate=0.0001, beta_1=0.98, beta_2=0.98), loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0VkGC7GtXtUc",
        "outputId": "7033a2dd-1dce-472e-c8f4-1c41b27d613d"
      },
      "source": [
        "### décommentez la ligne suivante si vous voulez entraîner votre propre modèle\n",
        "\n",
        "#history = modelX.fit(X_train, Y_train, epochs=10, batch_size=5, validation_split=0.2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "3074/3074 [==============================] - 13s 4ms/step - loss: 0.7413 - accuracy: 0.7063 - val_loss: 0.7294 - val_accuracy: 0.7158\n",
            "Epoch 2/10\n",
            "3074/3074 [==============================] - 13s 4ms/step - loss: 0.7210 - accuracy: 0.7111 - val_loss: 0.7182 - val_accuracy: 0.7184\n",
            "Epoch 3/10\n",
            "3074/3074 [==============================] - 13s 4ms/step - loss: 0.7377 - accuracy: 0.7067 - val_loss: 0.7215 - val_accuracy: 0.7234\n",
            "Epoch 4/10\n",
            "3074/3074 [==============================] - 13s 4ms/step - loss: 0.7168 - accuracy: 0.7058 - val_loss: 0.7186 - val_accuracy: 0.7200\n",
            "Epoch 5/10\n",
            "3074/3074 [==============================] - 12s 4ms/step - loss: 0.7401 - accuracy: 0.6998 - val_loss: 0.7211 - val_accuracy: 0.7252\n",
            "Epoch 6/10\n",
            "3074/3074 [==============================] - 13s 4ms/step - loss: 0.7284 - accuracy: 0.7073 - val_loss: 0.7246 - val_accuracy: 0.7192\n",
            "Epoch 7/10\n",
            "3074/3074 [==============================] - 13s 4ms/step - loss: 0.7163 - accuracy: 0.7169 - val_loss: 0.7223 - val_accuracy: 0.7177\n",
            "Epoch 8/10\n",
            "3074/3074 [==============================] - 13s 4ms/step - loss: 0.7292 - accuracy: 0.7121 - val_loss: 0.7245 - val_accuracy: 0.7184\n",
            "Epoch 9/10\n",
            "3074/3074 [==============================] - 13s 4ms/step - loss: 0.7155 - accuracy: 0.7136 - val_loss: 0.7239 - val_accuracy: 0.7203\n",
            "Epoch 10/10\n",
            "3074/3074 [==============================] - 13s 4ms/step - loss: 0.7219 - accuracy: 0.7105 - val_loss: 0.7177 - val_accuracy: 0.7250\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QhlBA7DuJuY1",
        "outputId": "9fafb886-9db7-4b7c-bffb-af7c0ba08c9f"
      },
      "source": [
        "### décommentez la ligne suivante si vous souhaitez enregistrer votre modèle personnalisé.\n",
        "\n",
        "#modelX.save('My_model')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: modelX/assets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UA7Sw0GNnojJ"
      },
      "source": [
        "Grâce au deuxième entrainement, nous parvenons à un score stable autour de  72%. Dans l'absolu, ce score peut ne pas paraître excellent, mais par rapport aux données utilisées c'est un score honorable.\n",
        "\n",
        "Vous pouvez essayer de relancer ce modèle (nous vous conseillons de le faire sur Colab ou avec un GPU) en jouant sur l'optimizer et ses hyperparamètre, ou même en modifiant le nombre de couches et de neurones par couches dans le modèle, vous obtiendrez peut-être un meilleur score que nous !"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85cZbrktdV4D"
      },
      "source": [
        "###### Validation sur la base de données de test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rshljthloKrX"
      },
      "source": [
        "Nous allons mainteant évaluer nos résulats grâce à nos données de test."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "igy5-FSWdV4E"
      },
      "source": [
        "modelX = models.load_model('modelX') \n",
        "#ici, nous vous proposons de charger notre modèle, mais vous pouvez faire les modifications nécessaires pour \n",
        "# utiliser le vôtre."
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TiGbt9WrdV4F",
        "outputId": "d9757185-d8d4-4e90-b839-e49f93f9534a"
      },
      "source": [
        "len(X_test)\n",
        "X_test.shape"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4810, 48, 48, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7HO9XCS3pUSR"
      },
      "source": [
        "Nous examiner les prédictions de notre algorithme sur les données de test, et les comparer à leur valeur réelle."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9BGTSXLNdV4N"
      },
      "source": [
        "Y_pred = modelX.predict(X_test) # on demande à notre modèle de réaliser les prédictions sur la base de test."
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-VGG4gWdV4O"
      },
      "source": [
        "y_pred=[]\n",
        "for y in Y_pred:\n",
        "    y_pred.append(np.argmax(y))\n",
        "# on décode les prédictions, afind e passer du format Y=[0,0,1,0] au format y=2, à l'inverse ce qu'on a afit plus haut. "
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NbYMS57UpqwT"
      },
      "source": [
        "L'utilisation d'une matrice de confusion matrice de confusion nous montrera où l'algorithme se \"trompe\". \n",
        "\n",
        "Dans l'idéal, toutes les valeurs devraient être sur la diagonale haut-gauche / bas-droite. Les valeurs qui se situent sur cette diagonale sont les images correctement identifiées. \n",
        "Les autres sont mal identifiées : les valeurs réelles se situent sur l'axe des ordonnées, et les valeurs prédites sur les abscisses."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WD5dN92YdV4P",
        "scrolled": true
      },
      "source": [
        "cf_matrix = confusion_matrix(y_test, y_pred) \n",
        "# le module metrics.confusion_matrix est disponible dnas la bibliothèque scikit-learn"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 389
        },
        "id": "YinsTYcKQbLs",
        "outputId": "645abedc-ae52-42ba-96dc-9e0340600c69"
      },
      "source": [
        "plt.figure(figsize=(6,6))\n",
        "plt.imshow(cf_matrix, cmap=\"Purples\")\n",
        "for (i, j), z in np.ndenumerate(cf_matrix):\n",
        "    plt.text(j, i, z, ha='center', va='center', bbox=dict(boxstyle='round', facecolor='white', edgecolor='0.3'))\n",
        "plt.xlabel(\"valeurs prédites\")\n",
        "plt.xticks(ticks =[i for i in range(4)], labels=[LABELS[i] for i in range(4)])\n",
        "plt.ylabel(\"valeurs réelles\")\n",
        "plt.yticks(ticks =[i for i in range(4)], labels=[LABELS[i] for i in range(4)])\n",
        "plt.show()"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ru3V3rZbdV4R"
      },
      "source": [
        "### <a id='5'>Résultats</a>\n",
        "\n",
        "La matrice de confusion nous montre  modèle semble a avoir plus de mal à reconnaître certains sentiments que d'autres. Comme on le voit sur la matrice de confusion que vous avez obtenue ci-dessus.\n",
        "\n",
        "Si l'on regarde les images de la colère, par exemple : un peu plus de la moitié d'entre elles ont dû être correctement identifiées, mais presque toutes les autres ont probablement été prises pour des expressions de la tristesse. Il y a d'ailleurs aussi beaucoup de mauvaises interprétations du sentiment de tristesse. \n",
        "\n",
        "Pas terrible... Qu'en pensez-vous ? \n",
        "Maintenant, allez regarder les images du dataset : à l'oeil nu, auriez-vous pu faire mieux ? ;)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8UKM3iy_1qJz"
      },
      "source": [
        ""
      ],
      "execution_count": 48,
      "outputs": []
    }
  ]
}